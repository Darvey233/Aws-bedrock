{"filter":false,"title":"rag_chatbot_lib.py","tooltip":"/workshop/labs/rag_chatbot/rag_chatbot_lib.py","undoManager":{"mark":6,"position":6,"stack":[[{"start":{"row":0,"column":0},"end":{"row":10,"column":0},"action":"insert","lines":["from langchain.memory import ConversationBufferWindowMemory","from langchain_community.chat_models import BedrockChat","from langchain.chains import ConversationalRetrievalChain","","from langchain_community.embeddings import BedrockEmbeddings","from langchain.indexes import VectorstoreIndexCreator","from langchain_community.vectorstores import FAISS","from langchain_text_splitters import RecursiveCharacterTextSplitter","from langchain_community.document_loaders import PyPDFLoader","",""],"id":1}],[{"start":{"row":10,"column":0},"end":{"row":26,"column":0},"action":"insert","lines":["def get_llm():","        ","    model_kwargs = { #anthropic","        \"max_tokens\": 512,","        \"temperature\": 0, ","        \"top_k\": 250, ","        \"top_p\": 1, ","        \"stop_sequences\": [\"\\n\\nHuman:\"] ","    }","    ","    llm = BedrockChat(","        model_id=\"anthropic.claude-3-sonnet-20240229-v1:0\", #set the foundation model","        model_kwargs=model_kwargs) #configure the inference parameters","    ","    return llm","",""],"id":2}],[{"start":{"row":26,"column":0},"end":{"row":50,"column":0},"action":"insert","lines":["def get_index(): #creates and returns an in-memory vector store to be used in the application","    ","    embeddings = BedrockEmbeddings() #create a Titan Embeddings client","    ","    pdf_path = \"2022-Shareholder-Letter.pdf\" #assumes local PDF file with this name","","    loader = PyPDFLoader(file_path=pdf_path) #load the pdf file","    ","    text_splitter = RecursiveCharacterTextSplitter( #create a text splitter","        separators=[\"\\n\\n\", \"\\n\", \".\", \" \"], #split chunks at (1) paragraph, (2) line, (3) sentence, or (4) word, in that order","        chunk_size=1000, #divide into 1000-character chunks using the separators above","        chunk_overlap=100 #number of characters that can overlap with previous chunk","    )","    ","    index_creator = VectorstoreIndexCreator( #create a vector store factory","        vectorstore_cls=FAISS, #use an in-memory vector store for demo purposes","        embedding=embeddings, #use Titan embeddings","        text_splitter=text_splitter, #use the recursive text splitter","    )","    ","    index_from_loader = index_creator.from_loaders([loader]) #create an vector store index from the loaded PDF","    ","    return index_from_loader #return the index to be cached by the client app","",""],"id":3}],[{"start":{"row":50,"column":0},"end":{"row":56,"column":0},"action":"insert","lines":["def get_memory(): #create memory for this chat session","    ","    memory = ConversationBufferWindowMemory(memory_key=\"chat_history\", return_messages=True) #Maintains a history of previous messages","    ","    return memory","",""],"id":4}],[{"start":{"row":49,"column":0},"end":{"row":50,"column":0},"action":"insert","lines":["",""],"id":5}],[{"start":{"row":57,"column":0},"end":{"row":58,"column":0},"action":"insert","lines":["",""],"id":6}],[{"start":{"row":58,"column":0},"end":{"row":68,"column":0},"action":"insert","lines":["def get_rag_chat_response(input_text, memory, index): #chat client function","    ","    llm = get_llm()","    ","    conversation_with_retrieval = ConversationalRetrievalChain.from_llm(llm, index.vectorstore.as_retriever(), memory=memory, verbose=True)","    ","    chat_response = conversation_with_retrieval.invoke({\"question\": input_text}) #pass the user message and summary to the model","    ","    return chat_response['answer']","",""],"id":7}]]},"ace":{"folds":[],"scrolltop":775,"scrollleft":0,"selection":{"start":{"row":68,"column":0},"end":{"row":68,"column":0},"isBackwards":false},"options":{"guessTabSize":true,"useWrapMode":false,"wrapToView":true},"firstLineState":0},"timestamp":1716766283990,"hash":"2b59ae4f8b7a9aee8048c0a3c0a92cd0e8e7559a"}